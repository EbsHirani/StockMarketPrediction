# -*- coding: utf-8 -*-
"""Stonks.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1IDDZKu71jYY7zNpPlHNW4PL0XSQXW6ES
"""

import pandas as pd
import datetime
import pandas_datareader.data as web
from pandas import Series, DataFrame


start = datetime.datetime(2017, 1, 30)
end = datetime.datetime(2019, 1, 30)

df = web.DataReader("AAPL", 'yahoo', start, end)
#df.tail()

SEQ = 60
FUTURE = 1
df["Future"] = df["Adj Close"].shift(-FUTURE)
#df.head()

return_df = df["Future"]/df["Adj Close"] - 1
#return_df.head()

# for cols in df.columns :
#   df[cols] = df[cols].pct_change()
from sklearn.preprocessing import MinMaxScaler
min_max_scaler = MinMaxScaler()
df = min_max_scaler.fit_transform(df)

#print(df)

df = df[:-1]
#print(df)

final_df = df.merge(return_df.rename("Returns"), left_index=True, right_index=True)
#final_df.head()

final_df.dropna(inplace = True)
#final_df.head()

from collections import deque
import numpy as np
abc = df
sequential_data = []
sequence = deque(maxlen = SEQ)
for dat in abc:
  sequence.append(dat[4:-1])
  if len(sequence) == SEQ:
    sequential_data.append([np.array(sequence),dat[-1]])

#sequential_data

import random
random.shuffle(sequential_data)

x = []
y = []
for xval , yval in sequential_data:
  x.append(xval)
  y.append(yval)
x = np.array(x)   
#x.shape

from sklearn.model_selection import train_test_split

xtrain,xval, ytrain, yval = train_test_split(x, y,test_size = 0.10, random_state = 42)

def standardise(j):
   return (j-xtrain.mean())/xtrain.std()

import pandas as pd 
import numpy as np
from keras.models import Sequential
from keras.layers import Dense , Dropout , Lambda, Flatten, LSTM, CuDNNLSTM
from keras.layers.normalization import BatchNormalization
from keras.optimizers import Adam ,RMSprop
from keras.optimizers import SGD
from sklearn.model_selection import train_test_split
from keras import  backend as K
from keras.utils.np_utils import to_categorical

from keras import optimizers

from keras.callbacks import EarlyStopping, TensorBoard, ModelCheckpoint
from keras.layers import BatchNormalization , MaxPooling2D, Activation
BATCH_SIZE = 16
TIME_STEPS = SEQ
lr = 0.005


model = Sequential([                   
                    CuDNNLSTM(64,  input_shape = (x.shape[1:])),
                    Dropout(0.2),
                    BatchNormalization(),
                    Dense(64, activation= 'relu'),
                    Dropout(0.2),
                    BatchNormalization(),
                    Dense(1,activation = 'linear')
])
model.compile(Adam(),loss='mean_squared_error', metrics=['accuracy'])
model.optimizer.lr=0.05
# lstm_model = Sequential()
# lstm_model.add(LSTM(100, batch_input_shape=(BATCH_SIZE, TIME_STEPS, x.shape[2]), dropout=0.0, recurrent_dropout=0.0, stateful=True,     kernel_initializer='random_uniform'))
# lstm_model.add(Dropout(0.5))
# lstm_model.add(Dense(20,activation='relu'))
# lstm_model.add(Dense(1,activation='sigmoid'))

# lstm_model.compile(RMSprop(lr), loss='mean_squared_error',metrics=['accuracy'])

# print(model.summary())

# print(xtrain)
# print(ytrain)

STEP_SIZE_TRAIN = xtrain.shape[0]//BATCH_SIZE
STEP_SIZE_VAL = xval.shape[0]//BATCH_SIZE

history = model.fit(xtrain,ytrain,batch_size=BATCH_SIZE,  epochs=20,validation_data=(xval,yval) )
#,callbacks=[EarlyStopping(monitor='val_loss', patience=3, verbose=1, mode='auto')]

print(model.predict(xtrain))

# print(xtrain)

# print(ytrain)